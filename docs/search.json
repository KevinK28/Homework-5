[
  {
    "objectID": "Homework5.html",
    "href": "Homework5.html",
    "title": "ST-558: Homework 5",
    "section": "",
    "text": "What is the purpose of using cross-validation when fitting a random forest model?\n\nThe purpose of using cross-validation when fitting a a random forest model is that it will give us a better idea of the \"best\" subset of predictors to use. Doing cross validation will lower our variance and provide a more robust result\n\nDescribe the bagged tree algorithm.\n\nThe bagged tree algorithm is is a general Bootstrap Aggregation method. For bagging, we will create a bootstrap sample (sample w/ replacement from original data) with the same number of observations as in the orinigal data. Then we would train a tree on this sample (no pruning necessary), and repeat many times (often 1,000). For each of these 1,000 samples, we will obtain our quantity/statistic of interest and then average them (or take majority vote for classification).\n\nWhat is meant by a general linear model?\n\nA general linear model is one that has a continous response and allows for both continuous and categorical predictors.\n\nWhen fitting a multiple linear regression model, what does adding an interaction term do? That is, what does it allow the model to do differently as compared to when it is not included in the model?\n\nAdding an interaction term to our mlr model will allow us to explore the combined effect of two or more predictor variables. More intuitively, the effect of one variable on the response is determined by the value of that variable and the value of another variable. When using an interaction term with a continuous and categorical variable, we can see how the effect of the continous variable changes for different levels of the categorical variable.\n\nWhy do we split our data into a training and test set?\n\nWe split our data into training and test sets because it allows us to more effectively check the robustness of our model. We use data to train a model to identify relationships and patterns; then use the test data to evaluate the performance of the model on \"unseen\" data. It gives us the chance to look at our model and see how it generalizes to new data."
  },
  {
    "objectID": "Homework5.html#eda-and-data-preparation",
    "href": "Homework5.html#eda-and-data-preparation",
    "title": "ST-558: Homework 5",
    "section": "EDA and Data Preparation",
    "text": "EDA and Data Preparation\nFirst, we will load in this dataset:\n\nlibrary(tidyverse)\n\nheart &lt;- read.csv(\"heart.csv\") |&gt;\n  as_tibble()\n\nhead(heart)\n\n# A tibble: 6 × 12\n    Age Sex   ChestPainType RestingBP Cholesterol FastingBS RestingECG MaxHR\n  &lt;int&gt; &lt;chr&gt; &lt;chr&gt;             &lt;int&gt;       &lt;int&gt;     &lt;int&gt; &lt;chr&gt;      &lt;int&gt;\n1    40 M     ATA                 140         289         0 Normal       172\n2    49 F     NAP                 160         180         0 Normal       156\n3    37 M     ATA                 130         283         0 ST            98\n4    48 F     ASY                 138         214         0 Normal       108\n5    54 M     NAP                 150         195         0 Normal       122\n6    39 M     NAP                 120         339         0 Normal       170\n# ℹ 4 more variables: ExerciseAngina &lt;chr&gt;, Oldpeak &lt;dbl&gt;, ST_Slope &lt;chr&gt;,\n#   HeartDisease &lt;int&gt;\n\n\nNow that we have successfully read in the data, let’s try to quickly understand some things about our data. We will check on the missingness and summarize the data (especially relationships of variables to heart disease).\n\n#Check the missingness\nmissing &lt;- colSums(is.na(heart))\nmissing\n\n           Age            Sex  ChestPainType      RestingBP    Cholesterol \n             0              0              0              0              0 \n     FastingBS     RestingECG          MaxHR ExerciseAngina        Oldpeak \n             0              0              0              0              0 \n      ST_Slope   HeartDisease \n             0              0 \n\n\nThere are no missing values in this data set (very unlikely in real-world), so let us start doing some data exploration/summarization\n\n#Check numerical variable summaries\nheartNumeric &lt;- heart |&gt;\n  select(where(is.numeric))\n\nsummary(heartNumeric)\n\n      Age          RestingBP      Cholesterol      FastingBS     \n Min.   :28.00   Min.   :  0.0   Min.   :  0.0   Min.   :0.0000  \n 1st Qu.:47.00   1st Qu.:120.0   1st Qu.:173.2   1st Qu.:0.0000  \n Median :54.00   Median :130.0   Median :223.0   Median :0.0000  \n Mean   :53.51   Mean   :132.4   Mean   :198.8   Mean   :0.2331  \n 3rd Qu.:60.00   3rd Qu.:140.0   3rd Qu.:267.0   3rd Qu.:0.0000  \n Max.   :77.00   Max.   :200.0   Max.   :603.0   Max.   :1.0000  \n     MaxHR          Oldpeak         HeartDisease   \n Min.   : 60.0   Min.   :-2.6000   Min.   :0.0000  \n 1st Qu.:120.0   1st Qu.: 0.0000   1st Qu.:0.0000  \n Median :138.0   Median : 0.6000   Median :1.0000  \n Mean   :136.8   Mean   : 0.8874   Mean   :0.5534  \n 3rd Qu.:156.0   3rd Qu.: 1.5000   3rd Qu.:1.0000  \n Max.   :202.0   Max.   : 6.2000   Max.   :1.0000  \n\n\nWe can see that in the RestingBP and Cholesterol variables, that they have a minimum of 0. This does not make any sense, and could actually be how they denote a missing value. Let us get rid of these observations.\n\nheart &lt;- heart |&gt;\n  filter(Cholesterol != 0 | RestingBP != 0)\n\nheartNumeric &lt;- heartNumeric |&gt;\n  filter(Cholesterol != 0 | RestingBP != 0)\n\nNow that we have gotten rid of the “missing” observations, we will now check the correlation between all numeric predictors and the HeartDisease variable:\n\n#Check correlation\ncor(heartNumeric$HeartDisease, heartNumeric[, -7])\n\n           Age RestingBP Cholesterol FastingBS      MaxHR  Oldpeak\n[1,] 0.2820117   0.11799  -0.2314786 0.2679937 -0.4014096 0.403638\n\n\nThis is interesting, none of the numeric variables have a very strong correlation to the HeartDisease variable. Let’s explore the non-numeric variables, first, through contingency tables:\n\nheartCategorical &lt;- heart |&gt;\n  select(!where(is.numeric), HeartDisease)\n\nfor (i in 1:ncol(heartCategorical)) {\n  contTable &lt;- table(heartCategorical$HeartDisease, as_vector(heartCategorical[,i]))\n  if (colnames(heartCategorical[,i]) != \"HeartDisease\") {\n    print(paste(\"Contingency Table for Heart Disease and\", colnames(heartCategorical[,i])))\n    print(contTable)\n    cat(\"\\n\")\n  } else {\n    NULL\n  }\n}\n\n[1] \"Contingency Table for Heart Disease and Sex\"\n   \n      F   M\n  0 143 267\n  1  50 457\n\n[1] \"Contingency Table for Heart Disease and ChestPainType\"\n   \n    ASY ATA NAP  TA\n  0 104 149 131  26\n  1 392  24  71  20\n\n[1] \"Contingency Table for Heart Disease and RestingECG\"\n   \n    LVH Normal  ST\n  0  82    267  61\n  1 106    284 117\n\n[1] \"Contingency Table for Heart Disease and ExerciseAngina\"\n   \n      N   Y\n  0 355  55\n  1 191 316\n\n[1] \"Contingency Table for Heart Disease and ST_Slope\"\n   \n    Down Flat  Up\n  0   14   79 317\n  1   49  380  78\n\n\nAfter looking at the contingency tables for each non-numeric variable and HeartDisease, we can see that many of these have a strong relationship with HeartDisease.\nNow that we are done with our exploration, we will start with our analyses. Before we begin, however, we will first need to do some extra data manipulation. Currently, our HeartDisease variable is numeric. We will need to change this to a factor variable, and also remove the ST_Slope variable and original HeartDisease variable.\n\nheart &lt;- heart |&gt;\n  mutate(HeartDiseaseFact = as.factor(HeartDisease)) |&gt;\n  select(-c(\"ST_Slope\", \"HeartDisease\"))\n\nhead(heart)\n\n# A tibble: 6 × 11\n    Age Sex   ChestPainType RestingBP Cholesterol FastingBS RestingECG MaxHR\n  &lt;int&gt; &lt;chr&gt; &lt;chr&gt;             &lt;int&gt;       &lt;int&gt;     &lt;int&gt; &lt;chr&gt;      &lt;int&gt;\n1    40 M     ATA                 140         289         0 Normal       172\n2    49 F     NAP                 160         180         0 Normal       156\n3    37 M     ATA                 130         283         0 ST            98\n4    48 F     ASY                 138         214         0 Normal       108\n5    54 M     NAP                 150         195         0 Normal       122\n6    39 M     NAP                 120         339         0 Normal       170\n# ℹ 3 more variables: ExerciseAngina &lt;chr&gt;, Oldpeak &lt;dbl&gt;,\n#   HeartDiseaseFact &lt;fct&gt;\n\n\nThe first analysis we will be doing involves a kNN model, where we generally want to have all numeric predictors. The next step will be to take our remaining categorical predictors, and create dummy variables that are numeric.\n\nlibrary(caret)\n\nheartCategorical &lt;- heartCategorical |&gt;\n  select(-c(\"ST_Slope\"))\n\ncatData &lt;- dummyVars(HeartDisease ~ ., data = heartCategorical) |&gt;\n  predict(newdata = heartCategorical)\n\nheartKNN &lt;- heart |&gt;\n  select(where(is.numeric), HeartDiseaseFact)\n\nheartKNN &lt;- cbind(heartKNN, catData) |&gt;\n  as_tibble()\n\nhead(heartKNN)\n\n# A tibble: 6 × 18\n    Age RestingBP Cholesterol FastingBS MaxHR Oldpeak HeartDiseaseFact  SexF\n  &lt;int&gt;     &lt;int&gt;       &lt;int&gt;     &lt;int&gt; &lt;int&gt;   &lt;dbl&gt; &lt;fct&gt;            &lt;dbl&gt;\n1    40       140         289         0   172     0   0                    0\n2    49       160         180         0   156     1   1                    1\n3    37       130         283         0    98     0   0                    0\n4    48       138         214         0   108     1.5 1                    1\n5    54       150         195         0   122     0   0                    0\n6    39       120         339         0   170     0   0                    0\n# ℹ 10 more variables: SexM &lt;dbl&gt;, ChestPainTypeASY &lt;dbl&gt;,\n#   ChestPainTypeATA &lt;dbl&gt;, ChestPainTypeNAP &lt;dbl&gt;, ChestPainTypeTA &lt;dbl&gt;,\n#   RestingECGLVH &lt;dbl&gt;, RestingECGNormal &lt;dbl&gt;, RestingECGST &lt;dbl&gt;,\n#   ExerciseAnginaN &lt;dbl&gt;, ExerciseAnginaY &lt;dbl&gt;"
  },
  {
    "objectID": "Homework5.html#split-data",
    "href": "Homework5.html#split-data",
    "title": "ST-558: Homework 5",
    "section": "Split Data",
    "text": "Split Data\nNow that we have gotten the correct data set, with dummy variables, we can now split the data into training and test sets. The training set will be comprised of 80% of the observations, and the test set will be the remaining 20% of the data.\n\n#set seed for reproducibility\nset.seed(100)\n\n#training/test sets\ntrain &lt;- sample(1:nrow(heartKNN), size = nrow(heartKNN)*0.8)\ntest  &lt;- dplyr::setdiff(1:nrow(heartKNN), train)\n\nheartKNNTrain &lt;- heart[train, ]\nheartKNNTest  &lt;- heart[test, ]\n\nWe have successfully split our data into training and testing data sets. Now, we will start to actually build/fit models"
  },
  {
    "objectID": "Homework5.html#k-nearest-neighbor-model",
    "href": "Homework5.html#k-nearest-neighbor-model",
    "title": "ST-558: Homework 5",
    "section": "k-Nearest-Neighbor Model",
    "text": "k-Nearest-Neighbor Model\nFor the first model, we will fit a kNN model. This will use a repeated 10 fold cross-validation, with the number of repeats being 3. We will also center and scale variables.\n\n#reproducibility\nset.seed(100)\n\ntrctrl &lt;- trainControl(method = \"repeatedcv\", number = 10, repeats = 3)\n\nkVals &lt;- data.frame(k = 1:40)\ntuneGridVals &lt;- data.frame(k = kVals)\n\nknnFit &lt;- train(HeartDiseaseFact ~ ., data = heartKNNTrain, method = \"knn\",\n                trControl = trctrl, preProcess = c(\"center\", \"scale\"),\n                tuneLength = 10,\n                tuneGrid = tuneGridVals)\n\nknnFit\n\nk-Nearest Neighbors \n\n733 samples\n 10 predictor\n  2 classes: '0', '1' \n\nPre-processing: centered (13), scaled (13) \nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 660, 660, 659, 660, 660, 660, ... \nResampling results across tuning parameters:\n\n  k   Accuracy   Kappa    \n   1  0.7871467  0.5722856\n   2  0.7834999  0.5647906\n   3  0.8048871  0.6075922\n   4  0.8062569  0.6110349\n   5  0.8208688  0.6399220\n   6  0.8131124  0.6246010\n   7  0.8208441  0.6403935\n   8  0.8199309  0.6388787\n   9  0.8231272  0.6455644\n  10  0.8240466  0.6472961\n  11  0.8272245  0.6536513\n  12  0.8240466  0.6473743\n  13  0.8331050  0.6655003\n  14  0.8331050  0.6654567\n  15  0.8372146  0.6737489\n  16  0.8367888  0.6725697\n  17  0.8354252  0.6698841\n  18  0.8340615  0.6670247\n  19  0.8363199  0.6716352\n  20  0.8340244  0.6669052\n  21  0.8340368  0.6667803\n  22  0.8336048  0.6656867\n  23  0.8331420  0.6650885\n  24  0.8313217  0.6609655\n  25  0.8322288  0.6626952\n  26  0.8327101  0.6632571\n  27  0.8326978  0.6631790\n  28  0.8317845  0.6609575\n  29  0.8318030  0.6609537\n  30  0.8295076  0.6562766\n  31  0.8345243  0.6658425\n  32  0.8281624  0.6528571\n  33  0.8295138  0.6556008\n  34  0.8290757  0.6541763\n  35  0.8276873  0.6513242\n  36  0.8254289  0.6464669\n  37  0.8263298  0.6483606\n  38  0.8313402  0.6586118\n  39  0.8277058  0.6510303\n  40  0.8281501  0.6520938\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was k = 15.\n\n\nWe have fit our model (k = 15), so now let’s make use of the test data set and look at the performance of our model.\n\nknnTestPred &lt;- predict(knnFit, newdata = heartKNNTest)\n\nconfusionMatrix(knnTestPred, heartKNNTest$HeartDiseaseFact)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 67 19\n         1 12 86\n                                          \n               Accuracy : 0.8315          \n                 95% CI : (0.7695, 0.8826)\n    No Information Rate : 0.5707          \n    P-Value [Acc &gt; NIR] : 4.032e-14       \n                                          \n                  Kappa : 0.6599          \n                                          \n Mcnemar's Test P-Value : 0.2812          \n                                          \n            Sensitivity : 0.8481          \n            Specificity : 0.8190          \n         Pos Pred Value : 0.7791          \n         Neg Pred Value : 0.8776          \n             Prevalence : 0.4293          \n         Detection Rate : 0.3641          \n   Detection Prevalence : 0.4674          \n      Balanced Accuracy : 0.8336          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nLooking at the performance of our kNN model, it looks pretty good! We see that we have an accuracy rate of 83.15%"
  },
  {
    "objectID": "Homework5.html#logistic-regression",
    "href": "Homework5.html#logistic-regression",
    "title": "ST-558: Homework 5",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nFor this section we will build three different logistic regression models. We can use the original heart data set (with HeartDisease as a factor and without ST_Slope variable). We will fit the three models on the training set and use repeated CV as above. We will not use centering and scaling here. After fitting the models, we will identify the best one and provide a basic summary of it.\n\n#set seed for reproducibility\nset.seed(100)\n\n#training/test sets\ntrain &lt;- sample(1:nrow(heart), size = nrow(heart)*0.8)\ntest  &lt;- dplyr::setdiff(1:nrow(heart), train)\n\nheartLogTrain &lt;- heart[train, ]\nheartLogTest  &lt;- heart[test, ]\n\n\nFirst Logistic Regression Model\nThe first model we will fit, will have all of the other variables as predictor variables:\n\nset.seed(100)\n\ntrctrl &lt;- trainControl(method = \"repeatedcv\", number = 10, repeats = 3)\n\nlogFitOne &lt;- train(HeartDiseaseFact ~ ., data = heartLogTrain, method = \"glm\",\n                   trControl = trctrl,\n                   tuneLength = 10)\n\nlogFitOne\n\nGeneralized Linear Model \n\n733 samples\n 10 predictor\n  2 classes: '0', '1' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 660, 660, 659, 660, 660, 660, ... \nResampling results:\n\n  Accuracy   Kappa   \n  0.8235468  0.643063\n\n\nThe accuracy of the model with all predictors is pretty good, but let us see if we can improve this at all.\n\n\nSecond Logisitic Regression Model\nThis next model will include al categorical variables, except for RestingECG, as well as cholesterol and age.\n\nset.seed(100)\n\ntrctrl &lt;- trainControl(method = \"repeatedcv\", number = 10, repeats = 3)\n\nlogFitTwo &lt;- train(HeartDiseaseFact ~ Age + Cholesterol + Sex + ChestPainType + ExerciseAngina,\n                   data = heartLogTrain, method = \"glm\",\n                   trControl = trctrl,\n                   tuneLength = 10)\n\nlogFitTwo\n\nGeneralized Linear Model \n\n733 samples\n  5 predictor\n  2 classes: '0', '1' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 660, 660, 659, 660, 660, 660, ... \nResampling results:\n\n  Accuracy   Kappa    \n  0.8008762  0.5965375\n\n\nThe accuracy of this model is pretty good once again; however, it is not as good as the first fit. Let’s try something else more complex.\n\n\nThird Logistic Regression Model\nFor the last logistic regression model, we will look at a model with all numeric variables.\n\nset.seed(100)\n\ntrctrl &lt;- trainControl(method = \"repeatedcv\", number = 10, repeats = 3)\n\nlogFitThree &lt;- train(HeartDiseaseFact ~ RestingBP + Cholesterol + FastingBS + MaxHR + Oldpeak +\n                       Age,\n                   data = heartLogTrain, method = \"glm\",\n                   trControl = trctrl,\n                   tuneLength = 10)\n\nlogFitThree\n\nGeneralized Linear Model \n\n733 samples\n  6 predictor\n  2 classes: '0', '1' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 660, 660, 659, 660, 660, 660, ... \nResampling results:\n\n  Accuracy   Kappa    \n  0.7726151  0.5418237\n\n\nThis model is the worst of the three.\n\n\nBest Model\nAfter fitting three models and checking the accuracy of each, the first model appears to be the “best”. Let’s provide a basic summary of it\n\nsummary(logFitOne)\n\n\nCall:\nNULL\n\nCoefficients:\n                  Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)      -0.522659   1.377524  -0.379 0.704376    \nAge               0.016250   0.013364   1.216 0.223977    \nSexM              1.071994   0.283704   3.779 0.000158 ***\nChestPainTypeATA -1.893897   0.315387  -6.005 1.91e-09 ***\nChestPainTypeNAP -1.665870   0.265279  -6.280 3.39e-10 ***\nChestPainTypeTA  -1.389389   0.454709  -3.056 0.002246 ** \nRestingBP         0.011831   0.006473   1.828 0.067602 .  \nCholesterol      -0.004603   0.001161  -3.963 7.39e-05 ***\nFastingBS         1.105520   0.274867   4.022 5.77e-05 ***\nRestingECGNormal -0.279206   0.284735  -0.981 0.326798    \nRestingECGST     -0.634758   0.373587  -1.699 0.089302 .  \nMaxHR            -0.012495   0.005008  -2.495 0.012596 *  \nExerciseAnginaY   1.343227   0.254358   5.281 1.29e-07 ***\nOldpeak           0.622460   0.118561   5.250 1.52e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1009.27  on 732  degrees of freedom\nResidual deviance:  554.87  on 719  degrees of freedom\nAIC: 582.87\n\nNumber of Fisher Scoring iterations: 5\n\n\nNow, let’s look at how well this model performs on the test set\n\nlogTestPred &lt;- predict(logFitOne, newdata = heartLogTest)\n\nconfusionMatrix(logTestPred, heartLogTest$HeartDiseaseFact)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 63 17\n         1 16 88\n                                          \n               Accuracy : 0.8207          \n                 95% CI : (0.7575, 0.8732)\n    No Information Rate : 0.5707          \n    P-Value [Acc &gt; NIR] : 5.171e-13       \n                                          \n                  Kappa : 0.6346          \n                                          \n Mcnemar's Test P-Value : 1               \n                                          \n            Sensitivity : 0.7975          \n            Specificity : 0.8381          \n         Pos Pred Value : 0.7875          \n         Neg Pred Value : 0.8462          \n             Prevalence : 0.4293          \n         Detection Rate : 0.3424          \n   Detection Prevalence : 0.4348          \n      Balanced Accuracy : 0.8178          \n                                          \n       'Positive' Class : 0"
  },
  {
    "objectID": "Homework5.html#tree-models",
    "href": "Homework5.html#tree-models",
    "title": "ST-558: Homework 5",
    "section": "Tree Models",
    "text": "Tree Models\nIn this section, we will look at a few different tree-based models. We are going to choose our own variable of interest (these models can accept factor/character variables) and we will use a 10-fold CV to select a best model.\n\nClassification Tree Model\nHere, we will be creating a classification tree model for the HeartDisease variable. To build this model, we will use all categorical predictors as well as Age, and Cholesterol\n\nset.seed(100)\n\n#training/test sets\ntrain &lt;- sample(1:nrow(heart), size = nrow(heart)*0.8)\ntest  &lt;- dplyr::setdiff(1:nrow(heart), train)\n\nheartTrain &lt;- heart[train, ]\nheartTest  &lt;- heart[test, ]\n\ntrctrl &lt;- trainControl(method = \"repeatedcv\", number = 10, repeats = 3)\ncpVals &lt;- data.frame(cp = seq(0, 0.1, 0.001))\ntuneGridVals &lt;- data.frame(cp = cpVals)\n\nclassTreeFit &lt;- train(HeartDiseaseFact ~ Age + Cholesterol + Sex + ChestPainType + RestingECG +\n                        ExerciseAngina, data = heartTrain,\n                      method = \"rpart\",\n                      trControl = trctrl,\n                      tuneGrid = tuneGridVals)\n\nclassTreeFit\n\nCART \n\n733 samples\n  6 predictor\n  2 classes: '0', '1' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 659, 660, 660, 659, 660, 660, ... \nResampling results across tuning parameters:\n\n  cp     Accuracy   Kappa    \n  0.000  0.7894395  0.5720570\n  0.001  0.7908094  0.5746786\n  0.002  0.7944439  0.5816565\n  0.003  0.7921731  0.5779257\n  0.004  0.7999170  0.5947549\n  0.005  0.8053964  0.6058447\n  0.006  0.8090186  0.6133607\n  0.007  0.8108450  0.6167367\n  0.008  0.8108450  0.6167367\n  0.009  0.8126530  0.6204689\n  0.010  0.8126530  0.6204689\n  0.011  0.8135541  0.6222043\n  0.012  0.8140045  0.6233017\n  0.013  0.8140045  0.6233017\n  0.014  0.8153620  0.6259543\n  0.015  0.8153620  0.6259543\n  0.016  0.8158187  0.6272326\n  0.017  0.8112895  0.6186161\n  0.018  0.8112895  0.6186161\n  0.019  0.8044402  0.6065307\n  0.020  0.8044402  0.6065307\n  0.021  0.8012500  0.6011758\n  0.022  0.8012500  0.6025440\n  0.023  0.8012500  0.6025440\n  0.024  0.8026075  0.6055921\n  0.025  0.8026075  0.6055921\n  0.026  0.8026075  0.6055921\n  0.027  0.8026075  0.6055921\n  0.028  0.8026075  0.6055921\n  0.029  0.8026075  0.6055921\n  0.030  0.8026075  0.6055921\n  0.031  0.8026075  0.6055921\n  0.032  0.8026075  0.6055921\n  0.033  0.8026075  0.6055921\n  0.034  0.8026075  0.6055921\n  0.035  0.8026075  0.6055921\n  0.036  0.8026075  0.6055921\n  0.037  0.8026075  0.6055921\n  0.038  0.8026075  0.6055921\n  0.039  0.8026075  0.6055921\n  0.040  0.8026075  0.6055921\n  0.041  0.8026075  0.6055921\n  0.042  0.8026075  0.6055921\n  0.043  0.8026075  0.6055921\n  0.044  0.8026075  0.6055921\n  0.045  0.8026075  0.6055921\n  0.046  0.8026075  0.6055921\n  0.047  0.8026075  0.6055921\n  0.048  0.8026075  0.6055921\n  0.049  0.8026075  0.6055921\n  0.050  0.8026075  0.6055921\n  0.051  0.8026075  0.6055921\n  0.052  0.8026075  0.6055921\n  0.053  0.8026075  0.6055921\n  0.054  0.8026075  0.6055921\n  0.055  0.8026075  0.6055921\n  0.056  0.8026075  0.6055921\n  0.057  0.8026075  0.6055921\n  0.058  0.8026075  0.6055921\n  0.059  0.8026075  0.6055921\n  0.060  0.8026075  0.6055921\n  0.061  0.8026075  0.6055921\n  0.062  0.8026075  0.6055921\n  0.063  0.8026075  0.6055921\n  0.064  0.8026075  0.6055921\n  0.065  0.8026075  0.6055921\n  0.066  0.8026075  0.6055921\n  0.067  0.8026075  0.6055921\n  0.068  0.8026075  0.6055921\n  0.069  0.8026075  0.6055921\n  0.070  0.8026075  0.6055921\n  0.071  0.8026075  0.6055921\n  0.072  0.8026075  0.6055921\n  0.073  0.8026075  0.6055921\n  0.074  0.8026075  0.6055921\n  0.075  0.8026075  0.6055921\n  0.076  0.8026075  0.6055921\n  0.077  0.8026075  0.6055921\n  0.078  0.8026075  0.6055921\n  0.079  0.8026075  0.6055921\n  0.080  0.8026075  0.6055921\n  0.081  0.8026075  0.6055921\n  0.082  0.8026075  0.6055921\n  0.083  0.8026075  0.6055921\n  0.084  0.8026075  0.6055921\n  0.085  0.8026075  0.6055921\n  0.086  0.8026075  0.6055921\n  0.087  0.8026075  0.6055921\n  0.088  0.8026075  0.6055921\n  0.089  0.8026075  0.6055921\n  0.090  0.8026075  0.6055921\n  0.091  0.8026075  0.6055921\n  0.092  0.8026075  0.6055921\n  0.093  0.8026075  0.6055921\n  0.094  0.8026075  0.6055921\n  0.095  0.8026075  0.6055921\n  0.096  0.8026075  0.6055921\n  0.097  0.8026075  0.6055921\n  0.098  0.8026075  0.6055921\n  0.099  0.8026075  0.6055921\n  0.100  0.8026075  0.6055921\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was cp = 0.016.\n\n\n\n\nRandom Forest Model\nHere, we will build a random forest model, using all of the variables in the data set as predictors\n\nset.seed(100)\n\ntrtctrl &lt;- trctrl &lt;- trainControl(method = \"repeatedcv\", number = 10, repeats = 3)\nrfVals &lt;- data.frame(mtry = seq(1,10))\ntuneGridVals &lt;- data.frame(mtry = rfVals)\n\nrfTreeFit &lt;- train(HeartDiseaseFact ~ ., data = heartTrain,\n                   method = \"rf\",\n                   trControl = trctrl,\n                   tuneGrid = tuneGridVals)\n\nrfTreeFit\n\nRandom Forest \n\n733 samples\n 10 predictor\n  2 classes: '0', '1' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 660, 660, 659, 660, 660, 660, ... \nResampling results across tuning parameters:\n\n  mtry  Accuracy   Kappa    \n   1    0.8326669  0.6607527\n   2    0.8354313  0.6678370\n   3    0.8395162  0.6757858\n   4    0.8317969  0.6602196\n   5    0.8290571  0.6545535\n   6    0.8322535  0.6611702\n   7    0.8249722  0.6466303\n   8    0.8276749  0.6518898\n   9    0.8249599  0.6470285\n  10    0.8235900  0.6440622\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was mtry = 3.\n\n\n\n\nBoosted Tree Model\nHere, we will run a boosted tree model using only the categorical variables in the model.\n\nset.seed(100)\n\ntrtctrl &lt;- trctrl &lt;- trainControl(method = \"repeatedcv\", number = 10, repeats = 3)\ntuneGridVals &lt;- expand.grid(n.trees = c(25, 50, 100, 200),\n                            interaction.depth = c(1, 2, 3),\n                            shrinkage = 0.1,\n                            n.minobsinnode = 10)\n\nboostTreeFit &lt;- train(HeartDiseaseFact ~ Sex + ChestPainType + ExerciseAngina + RestingECG,\n                      data = heartTrain,\n                      method = \"gbm\",\n                      trControl = trctrl,\n                      tuneGrid = tuneGridVals,\n                      verbose = FALSE)\n\nboostTreeFit\n\nStochastic Gradient Boosting \n\n733 samples\n  4 predictor\n  2 classes: '0', '1' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 660, 660, 659, 660, 660, 660, ... \nResampling results across tuning parameters:\n\n  interaction.depth  n.trees  Accuracy   Kappa    \n  1                   25      0.8013390  0.5948790\n  1                   50      0.7990744  0.5907299\n  1                  100      0.7959089  0.5861471\n  1                  200      0.7999877  0.5947000\n  2                   25      0.7977169  0.5875450\n  2                   50      0.7967913  0.5874830\n  2                  100      0.7949772  0.5843442\n  2                  200      0.7967975  0.5884931\n  3                   25      0.7968160  0.5860573\n  3                   50      0.7972603  0.5883105\n  3                  100      0.7931692  0.5801863\n  3                  200      0.7908984  0.5755458\n\nTuning parameter 'shrinkage' was held constant at a value of 0.1\n\nTuning parameter 'n.minobsinnode' was held constant at a value of 10\nAccuracy was used to select the optimal model using the largest value.\nThe final values used for the model were n.trees = 25, interaction.depth =\n 1, shrinkage = 0.1 and n.minobsinnode = 10.\n\n\n\n\nResults\nHere, we will look at the results of the final model for each of the three tree-based methods.\n\n#Create Predictions\nclassTestPred &lt;- predict(classTreeFit, newdata = heartTest)\nrfTestPred    &lt;- predict(rfTreeFit, newdata = heartTest)\nboostTestPred &lt;- predict(boostTreeFit, newdata = heartTest)\n\nLet’s first look at the classification tree performance:\n\nconfusionMatrix(classTestPred, heartTest$HeartDiseaseFact)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 48 21\n         1 31 84\n                                          \n               Accuracy : 0.7174          \n                 95% CI : (0.6465, 0.7812)\n    No Information Rate : 0.5707          \n    P-Value [Acc &gt; NIR] : 2.796e-05       \n                                          \n                  Kappa : 0.4141          \n                                          \n Mcnemar's Test P-Value : 0.212           \n                                          \n            Sensitivity : 0.6076          \n            Specificity : 0.8000          \n         Pos Pred Value : 0.6957          \n         Neg Pred Value : 0.7304          \n             Prevalence : 0.4293          \n         Detection Rate : 0.2609          \n   Detection Prevalence : 0.3750          \n      Balanced Accuracy : 0.7038          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nRandom Forest Performance:\n\nconfusionMatrix(rfTestPred, heartTest$HeartDiseaseFact)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 62 18\n         1 17 87\n                                          \n               Accuracy : 0.8098          \n                 95% CI : (0.7455, 0.8638)\n    No Information Rate : 0.5707          \n    P-Value [Acc &gt; NIR] : 5.74e-12        \n                                          \n                  Kappa : 0.6124          \n                                          \n Mcnemar's Test P-Value : 1               \n                                          \n            Sensitivity : 0.7848          \n            Specificity : 0.8286          \n         Pos Pred Value : 0.7750          \n         Neg Pred Value : 0.8365          \n             Prevalence : 0.4293          \n         Detection Rate : 0.3370          \n   Detection Prevalence : 0.4348          \n      Balanced Accuracy : 0.8067          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nBoosted Tree Performance:\n\nconfusionMatrix(boostTestPred, heartTest$HeartDiseaseFact)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 52 13\n         1 27 92\n                                         \n               Accuracy : 0.7826         \n                 95% CI : (0.716, 0.8399)\n    No Information Rate : 0.5707         \n    P-Value [Acc &gt; NIR] : 1.306e-09      \n                                         \n                  Kappa : 0.5464         \n                                         \n Mcnemar's Test P-Value : 0.03983        \n                                         \n            Sensitivity : 0.6582         \n            Specificity : 0.8762         \n         Pos Pred Value : 0.8000         \n         Neg Pred Value : 0.7731         \n             Prevalence : 0.4293         \n         Detection Rate : 0.2826         \n   Detection Prevalence : 0.3533         \n      Balanced Accuracy : 0.7672         \n                                         \n       'Positive' Class : 0"
  },
  {
    "objectID": "Homework5.html#wrap-up",
    "href": "Homework5.html#wrap-up",
    "title": "ST-558: Homework 5",
    "section": "Wrap Up",
    "text": "Wrap Up\nAfter reviewing the results of all the models fit above, we can see that the kNN model had the best results (in terms of accuracy)."
  }
]