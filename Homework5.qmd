---
title: "ST-558: Homework 5"
author: "Kevin Krupa"
format: html
editor: visual
---

# Task 1: Conceptual Questions

1. **What is the purpose of using cross-validation when fitting a random forest model?**

  `The purpose of using cross-validation when fitting a a random forest model is that it will give us a better idea of the "best" subset of predictors to use. Doing cross validation will lower our variance and provide a more robust result`

2. **Describe the bagged tree algorithm.**

  `The bagged tree algorithm is is a general Bootstrap Aggregation method. For bagging, we will create a bootstrap sample (sample w/ replacement from original data) with the same number of observations as in the orinigal data. Then we would train a tree on this sample (no pruning necessary), and repeat many times (often 1,000). For each of these 1,000 samples, we will obtain our quantity/statistic of interest and then average them (or take majority vote for classification).` 

3. **What is meant by a general linear model?**

  `A general linear model is one that has a continous response and allows for both continuous and categorical predictors.`

4. **When fitting a multiple linear regression model, what does adding an interaction term do? That is, what does it allow the model to do differently as compared to when it is not included in the model?**

  `Adding an interaction term to our mlr model will allow us to explore the combined effect of two or more predictor variables. More intuitively, the effect of one variable on the response is determined by the value of that variable and the value of another variable. When using an interaction term with a continuous and categorical variable, we can see how the effect of the continous variable changes for different levels of the categorical variable.`

5. **Why do we split our data into a training and test set?**

  `We split our data into training and test sets because it allows us to more effectively check the robustness of our model. We use data to train a model to identify relationships and patterns; then use the test data to evaluate the performance of the model on "unseen" data. It gives us the chance to look at our model and see how it generalizes to new data.`


# Task 2: Fitting Models

In this task, we will be looking at a specific data set: `heart.csv`. This data set gives information about whether or not an individual has heart disease (`HeartDisease = 1/0`). It also gives more information about an individual's health. Here is a link with more information about the data set: <a href="https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction">Heart Disease</a>

## EDA and Data Preparation

First, we will load in this dataset:

```{r, warning = FALSE, message = FALSE}
library(tidyverse)

heart <- read.csv("heart.csv") |>
  as_tibble()

head(heart)
```

Now that we have successfully read in the data, let's try to quickly understand some things about our data. We will check on the missingness and summarize the data (especially relationships of variables to heart disease).

```{r}
#Check the missingness
missing <- colSums(is.na(heart))
missing
```
There are no missing values in this data set (very unlikely in real-world), so let us start doing some data exploration/summarization

```{r}
#Check numerical variable summaries
heartNumeric <- heart |>
  select(where(is.numeric))

summary(heartNumeric)
```
We can see that in the `RestingBP` and `Cholesterol` variables, that they have a minimum of 0. This does not make any sense, and could actually be how they denote a missing value. Let us get rid of these observations.

```{r}
heart <- heart |>
  filter(Cholesterol != 0 | RestingBP != 0)

heartNumeric <- heartNumeric |>
  filter(Cholesterol != 0 | RestingBP != 0)
```

Now that we have gotten rid of the *"missing"* observations, we will now check the correlation between all numeric predictors and the `HeartDisease` variable:

```{r}
#Check correlation
cor(heartNumeric$HeartDisease, heartNumeric[, -7])
```
This is interesting, none of the numeric variables have a very strong correlation to the `HeartDisease` variable. Let's explore the non-numeric variables, first, through contingency tables:

```{r}
heartCategorical <- heart |>
  select(!where(is.numeric), HeartDisease)

for (i in 1:ncol(heartCategorical)) {
  contTable <- table(heartCategorical$HeartDisease, as_vector(heartCategorical[,i]))
  if (colnames(heartCategorical[,i]) != "HeartDisease") {
    print(paste("Contingency Table for Heart Disease and", colnames(heartCategorical[,i])))
    print(contTable)
    cat("\n")
  } else {
    NULL
  }
}
```

After looking at the contingency tables for each non-numeric variable and `HeartDisease`, we can see that many of these have a strong relationship with `HeartDisease`.

Now that we are done with our exploration, we will start with our analyses. Before we begin, however, we will first need to do some extra data manipulation. Currently, our `HeartDisease` variable is numeric. We will need to change this to a factor variable, and also remove the `ST_Slope` variable and original `HeartDisease` variable.

```{r}
heart <- heart |>
  mutate(HeartDiseaseFact = as.factor(HeartDisease)) |>
  select(-c("ST_Slope", "HeartDisease"))

head(heart)
```

The first analysis we will be doing involves a kNN model, where we generally want to have all numeric predictors. The next step will be to take our remaining categorical predictors, and create dummy variables that are numeric.

```{r, warning = FALSE, message = FALSE}
library(caret)

heartCategorical <- heartCategorical |>
  select(-c("ST_Slope"))

catData <- dummyVars(HeartDisease ~ ., data = heartCategorical) |>
  predict(newdata = heartCategorical)

heartKNN <- heart |>
  select(where(is.numeric), HeartDiseaseFact)

heartKNN <- cbind(heartKNN, catData) |>
  as_tibble()

head(heartKNN)
```

## Split Data

Now that we have gotten the correct data set, with dummy variables, we can now split the data into training and test sets. The training set will be comprised of 80% of the observations, and the test set will be the remaining 20% of the data.

```{r}
#set seed for reproducibility
set.seed(100)

#training/test sets
train <- sample(1:nrow(heartKNN), size = nrow(heartKNN)*0.8)
test  <- dplyr::setdiff(1:nrow(heartKNN), train)

heartKNNTrain <- heart[train, ]
heartKNNTest  <- heart[test, ]
```

We have successfully split our data into training and testing data sets. Now, we will start to actually build/fit models

## k-Nearest-Neighbor Model

For the first model, we will fit a kNN model. This will use a repeated 10 fold cross-validation, with the number of repeats being 3. We will also center and scale variables.

```{r}
#reproducibility
set.seed(100)

trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)

kVals <- data.frame(k = 1:40)
tuneGridVals <- data.frame(k = kVals)

knnFit <- train(HeartDiseaseFact ~ ., data = heartKNNTrain, method = "knn",
                trControl = trctrl, preProcess = c("center", "scale"),
                tuneLength = 10,
                tuneGrid = tuneGridVals)

knnFit
```

We have fit our model (k = 15), so now let's make use of the test data set and look at the performance of our model.

```{r}
knnTestPred <- predict(knnFit, newdata = heartKNNTest)

confusionMatrix(knnTestPred, heartKNNTest$HeartDiseaseFact)
```
Looking at the performance of our kNN model, it looks pretty good! We see that we have an accuracy rate of 83.15%

## Logistic Regression

For this section we will build three different logistic regression models. We can use the original heart data set (with `HeartDisease` as a factor and without `ST_Slope` variable). We will fit the three models on the training set and use repeated CV as above. We will not use centering and scaling here. After fitting the models, we will identify the best one and provide a basic summary of it.

```{r}
#set seed for reproducibility
set.seed(100)

#training/test sets
train <- sample(1:nrow(heart), size = nrow(heart)*0.8)
test  <- dplyr::setdiff(1:nrow(heart), train)

heartLogTrain <- heart[train, ]
heartLogTest  <- heart[test, ]
```

### First Logistic Regression Model

The first model we will fit, will have all of the other variables as predictor variables:

```{r}
set.seed(100)

trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)

logFitOne <- train(HeartDiseaseFact ~ ., data = heartLogTrain, method = "glm",
                   trControl = trctrl,
                   tuneLength = 10)

logFitOne
```
The accuracy of the model with all predictors is pretty good, but let us see if we can improve this at all.

### Second Logisitic Regression Model

This next model will include al categorical variables, except for `RestingECG`, as well as cholesterol and age.

```{r}
set.seed(100)

trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)

logFitTwo <- train(HeartDiseaseFact ~ Age + Cholesterol + Sex + ChestPainType + ExerciseAngina,
                   data = heartLogTrain, method = "glm",
                   trControl = trctrl,
                   tuneLength = 10)

logFitTwo
```
The accuracy of this model is pretty good once again; however, it is not as good as the first fit. Let's try something else more complex.

### Third Logistic Regression Model

For the last logistic regression model, we will look at a model with all numeric variables.

```{r}
set.seed(100)

trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)

logFitThree <- train(HeartDiseaseFact ~ RestingBP + Cholesterol + FastingBS + MaxHR + Oldpeak +
                       Age,
                   data = heartLogTrain, method = "glm",
                   trControl = trctrl,
                   tuneLength = 10)

logFitThree
```
This model is the worst of the three.

### Best Model

After fitting three models and checking the accuracy of each, the first model appears to be the "best". Let's provide a basic summary of it

```{r}
summary(logFitOne)
```
Now, let's look at how well this model performs on the test set

```{r}
logTestPred <- predict(logFitOne, newdata = heartLogTest)

confusionMatrix(logTestPred, heartLogTest$HeartDiseaseFact)
```
## Tree Models

In this section, we will look at a few different tree-based models. We are going to choose our own variable of interest (these models can accept factor/character variables) and we will use a 10-fold CV to select a best model.

### Classification Tree Model

Here, we will be creating a classification tree model for the `HeartDisease` variable. To build this model, we will use all categorical predictors as well as Age, and Cholesterol

```{r}
set.seed(100)

#training/test sets
train <- sample(1:nrow(heart), size = nrow(heart)*0.8)
test  <- dplyr::setdiff(1:nrow(heart), train)

heartTrain <- heart[train, ]
heartTest  <- heart[test, ]

trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
cpVals <- data.frame(cp = seq(0, 0.1, 0.001))
tuneGridVals <- data.frame(cp = cpVals)

classTreeFit <- train(HeartDiseaseFact ~ Age + Cholesterol + Sex + ChestPainType + RestingECG +
                        ExerciseAngina, data = heartTrain,
                      method = "rpart",
                      trControl = trctrl,
                      tuneGrid = tuneGridVals)

classTreeFit
```


### Random Forest Model

Here, we will build a random forest model, using all of the variables in the data set as predictors

```{r}
set.seed(100)

trtctrl <- trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
rfVals <- data.frame(mtry = seq(1,10))
tuneGridVals <- data.frame(mtry = rfVals)

rfTreeFit <- train(HeartDiseaseFact ~ ., data = heartTrain,
                   method = "rf",
                   trControl = trctrl,
                   tuneGrid = tuneGridVals)

rfTreeFit
```


### Boosted Tree Model

Here, we will run a boosted tree model using only the categorical variables in the model.

```{r}
set.seed(100)

trtctrl <- trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
tuneGridVals <- expand.grid(n.trees = c(25, 50, 100, 200),
                            interaction.depth = c(1, 2, 3),
                            shrinkage = 0.1,
                            n.minobsinnode = 10)

boostTreeFit <- train(HeartDiseaseFact ~ Sex + ChestPainType + ExerciseAngina + RestingECG,
                      data = heartTrain,
                      method = "gbm",
                      trControl = trctrl,
                      tuneGrid = tuneGridVals,
                      verbose = FALSE)

boostTreeFit
```

### Results

Here, we will look at the results of the final model for each of the three tree-based methods.

```{r}
#Create Predictions
classTestPred <- predict(classTreeFit, newdata = heartTest)
rfTestPred    <- predict(rfTreeFit, newdata = heartTest)
boostTestPred <- predict(boostTreeFit, newdata = heartTest)

```

Let's first look at the classification tree performance:

```{r}
confusionMatrix(classTestPred, heartTest$HeartDiseaseFact)
```

Random Forest Performance:

```{r}
confusionMatrix(rfTestPred, heartTest$HeartDiseaseFact)
```
Boosted Tree Performance:

```{r}
confusionMatrix(boostTestPred, heartTest$HeartDiseaseFact)
```

## Wrap Up

After reviewing the results of all the models fit above, we can see that the kNN model had the best results (in terms of accuracy).